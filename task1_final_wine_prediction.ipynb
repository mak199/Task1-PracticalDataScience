{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:Advance Practical Data Science\n",
    "First data science problem: given a list of wine characteristics, the goal is to predict the quality of that wine.\n",
    "\n",
    "Here, the quality of a wine is an integer value in the range of 0 (very bad) to 10 (very good). In the train data this quality is given for each wine. Your task is to predict this value for the test data.\n",
    "\n",
    "Please follow the steps discussed in the lectures to approach the task.\n",
    "\n",
    "1.Perform Exploratory Data Analysis (EDA) on the data.\n",
    "\n",
    "2.Based on the EDA, build a model that predicts wine quality for the wines listed in the test set.\n",
    "\n",
    "3.Submit your solutions to this competition website. The scoring metric is the Mean Absolute Error (MAE).\n",
    "\n",
    "4.Prepare a presentation, containing your main findings in EDA, how you translated them to your predictive model, and an evaluation of your model. Present your results in class on November 16th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Outline\n",
    "1.Wrangle Data\n",
    "\n",
    "2.Preprocess data\n",
    "\n",
    "3.Evaluation and analysis\n",
    "\n",
    "4.Visualization\n",
    "\n",
    "5.Importance of features\n",
    "\n",
    "6.Train Model\n",
    "\n",
    "7.Predict on Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:Load wine quality dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_data(train_file,test_file):\n",
    "    # Read in data as a dataframe\n",
    "    original_train_data = pd.read_csv('train_data.csv')\n",
    "    original_test_data = pd.read_csv(\"test_data.csv\")\n",
    "    return [original_train_data,original_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data(train_data):\n",
    "    #data_train.isnull().sum()\n",
    "    #missing values\n",
    "    missing_data = train_data[train_data.isnull()==True].count()\n",
    "    print(\"Overall empty data fields:\",missing_data.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know the features of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset(data,feature):\n",
    "    data.columns\n",
    "    # Descriptive statistics for each column\n",
    "    #features.describe()\n",
    "    print(round(data.describe(), 2))\n",
    "    print(data[feature].unique())\n",
    "    print(data[feature].value_counts())\n",
    "    print(data[feature].describe())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary of features of data,such as distribution(mean,min,max and standard deviation)\n",
    "Analyse the features to indentify any anomalies in the scaling of features,such as density of pH being too high.\n",
    "Also compare feature scaling with respect to eachother"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out about the target variable of the dataset,if it is continuous or discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out about the frequency of the discrete target values of the wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about the target values of the wine data set(distribution of a feature explains alot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of the target variable of the dataset explains that some of the quality values do not appear often enough for the model to properly learn features and pattern leading to those quality values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_histogram(features_train):\n",
    "    import matplotlib.pyplot as plt\n",
    "    rng = features_train['quality']\n",
    "    plt.hist(rng, bins='auto')  # arguments are passed to np.histogram\n",
    "    plt.title(\"Frequency of range of Quality\")\n",
    "    plt.xlabel(\"Quality\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    #plt.savefig(\"hist.png\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation of the features explains how relationship between two features.\n",
    "Correlation is a measure that determines the degree to which two variables' movements are associated.\n",
    "-1.0 indicates a perfect negative correlation, and a correlation of 1.0 indicates a perfect positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_corr(features_train):\n",
    "    #Correlation of features\n",
    "    cmat = features_train.corr()\n",
    "    f, ax = plt.subplots(figsize=(10,5))\n",
    "    sns.heatmap(cmat,square=True)\n",
    "    f.tight_layout()\n",
    "    #plt.savefig(\"corr.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA using boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_boxplot(features_train):\n",
    "    #https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "    #https://seaborn.pydata.org/tutorial/axis_grids.html#plotting-small-multiples-of-data-subsets\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    #data_train['fixed acidity'] = np.log10(data_train['fixed acidity'])\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    ax = sns.boxplot(x=\"quality\", y=\"fixed acidity\", data=features_train)\n",
    "    plt.savefig(\"fixed_acid_boxplot.png\")\n",
    "    plt.figure()\n",
    "    #data_train['volatile acidity'] = np.log10(data_train['volatile acidity'])\n",
    "    ax = sns.boxplot(x=\"quality\", y=\"volatile acidity\", data=features_train)\n",
    "    plt.savefig(\"volatile_acid_boxplot.png\")\n",
    "    plt.figure()\n",
    "    #data_train['total sulfur dioxide'] = np.log10(data_train['total sulfur dioxide'])\n",
    "    ax = sns.boxplot(x=\"quality\", y=\"total sulfur dioxide\", data=features_train)\n",
    "    plt.savefig(\"sulfur_dioxide_boxplot.png\")\n",
    "    plt.figure()\n",
    "    #data_train['pH'] = np.log10(data_train['pH'])\n",
    "    ax = sns.boxplot(x=\"quality\", y=\"pH\", data=features_train)\n",
    "    plt.savefig(\"pH_boxplot.png\")\n",
    "    plt.figure()\n",
    "    #data_train['alcohol'] = np.log10(data_train['alcohol'])\n",
    "    ax = sns.boxplot(x=\"quality\", y=\"alcohol\", data=features_train)\n",
    "    plt.savefig(\"alcohol_boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA with barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_barplot(features_train):\n",
    "    #Here we see that fixed acidity does not give any specification to classify the quality.\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'fixed acidity', data = features_train)\n",
    "    plt.savefig(\"fixed_acid_barplot.png\")\n",
    "    \n",
    "    #Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'volatile acidity', data = features_train)\n",
    "    plt.savefig(\"volatile_acid_barplot.png\")\n",
    "    \n",
    "    #Composition of citric acid go higher as we go higher in the quality of the wine\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'citric acid', data = features_train)\n",
    "    plt.savefig(\"citric_acid_barplot.png\")\n",
    "    \n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'residual sugar', data = features_train)\n",
    "    plt.savefig(\"residual_sugar_barplot.png\")\n",
    "\n",
    "    #Composition of chloride also go down as we go higher in the quality of the wine\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'chlorides', data = features_train)\n",
    "    plt.savefig(\"chlorides_barplot.png\")\n",
    "\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'free sulfur dioxide', data = features_train)\n",
    "    plt.savefig(\"sulfur_dioxide_barplot.png\")\n",
    "\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'total sulfur dioxide', data = features_train)\n",
    "    plt.savefig(\"sulfur_dioxide_barplot.png\")\n",
    "    \n",
    "    #Alcohol level also goes higher as te quality of wine increases\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    sns.barplot(x = 'quality', y = 'alcohol', data = features_train)\n",
    "    plt.savefig(\"alcohol_barplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA using Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDF_Pairplot(features_train):\n",
    "\n",
    "    print(\"Plot 1\")\n",
    "    g = sns.pairplot(features_train,vars=[\"quality\",\"alcohol\",\"volatile acidity\",\"free sulfur dioxide\"], hue=\"quality\", palette=\"Set2\", diag_kind=\"kde\", size=2.5)\n",
    "    plt.savefig(\"pairlot1.png\")\n",
    "    print(\"Plot 2\")\n",
    "    g = sns.pairplot(features_train,vars=[\"quality\",\"citric acid\",\"fixed acidity\",\"pH\"], hue=\"quality\", palette=\"Set2\", diag_kind=\"kde\", size=2.5)\n",
    "    plt.savefig(\"pairlot2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:Spliting and preparing data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to for data conversion such as one hot encoding(data already numeric).\n",
    "Extract labels and features from dataset.\n",
    "Split dataset into train and test(labels and features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_test_train(data,ratio):\n",
    "    # Extract features and labels\n",
    "    labels = data['quality']\n",
    "    features = data.drop('quality', axis = 1)\n",
    "\n",
    "    # List of features for later use\n",
    "    feature_list = list(features.columns)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features)\n",
    "\n",
    "    # Training and Testing Sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                                test_size = ratio, random_state = 42)\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "    \n",
    "    return [train_features, test_features, train_labels, test_labels, feature_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine importance of features using base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train base RandomForestRegressor algorithm using training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_feature_importance(features,labels):\n",
    "    # Import the model we are using\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Instantiate model \n",
    "    rfr = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rfr.fit(features, labels)\n",
    "    \n",
    "    # Get numerical feature importances\n",
    "    importances = list(rfr.feature_importances_)\n",
    " \n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    # Print out the feature and importances \n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    # Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "    import matplotlib.pyplot as plt\n",
    "    # list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "\n",
    "    # Make a bar chart\n",
    "    plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "    # Tick labels for x axis\n",
    "    plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "    # Axis labels and title\n",
    "    plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n",
    "    \n",
    "    #------------------------------------------------------------------------------------\n",
    "   \n",
    "    # List of features sorted from most to least important\n",
    "    sorted_importances = [importance[1] for importance in feature_importances]\n",
    "    sorted_features = [importance[0] for importance in feature_importances]\n",
    "\n",
    "    # Cumulative importances\n",
    "    cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "    # Make a line graph\n",
    "    plt.plot(x_values, cumulative_importances, 'g-')\n",
    "\n",
    "    # Draw line at 95% of importance retained\n",
    "    plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "\n",
    "    # Format x ticks and labels\n",
    "    plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "\n",
    "    # Axis labels and title\n",
    "    plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');\n",
    "\n",
    "    # Find number of features for cumulative importance of 95%\n",
    "    # Add 1 because Python is zero-indexed\n",
    "    print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the importance of features in the wine dataset using base RandomForest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the importance of features in a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative importance of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 features from the dataset have more than 95 percent importance in the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restrict to the Most Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature \"type\" and \"density\" are not very important for the training of the model\n",
    "Therefore these two features are left out from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build test and train data with important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_features_of_data(train_features,test_features,feature_list):\n",
    "    # Names of five importances accounting for 95% of total importance\n",
    "    important_feature_names = ['fixed acidity',\n",
    "     'volatile acidity',\n",
    "     'citric acid',\n",
    "     'residual sugar',\n",
    "     'chlorides',\n",
    "     'free sulfur dioxide',\n",
    "     'total sulfur dioxide',\n",
    "     'pH',\n",
    "     'sulphates',\n",
    "     'alcohol']\n",
    "       \n",
    "        \n",
    "    # Find the columns of the most important features\n",
    "    important_indices = [feature_list.index(feature) for feature in important_feature_names]\n",
    "\n",
    "    # Create training and testing sets with only the important features\n",
    "    important_train_features = train_features[:, important_indices]\n",
    "    important_test_features = test_features[:, important_indices]\n",
    "\n",
    "    # Sanity check on operations\n",
    "    print('Important train features shape:', important_train_features.shape)\n",
    "    print('Important test features shape:', important_test_features.shape)\n",
    "    \n",
    "    # Use only the most important features\n",
    "    train_important_features = important_train_features[:]\n",
    "    test_important_features = important_test_features[:]\n",
    "\n",
    "    # Update feature list for visualizations\n",
    "    feature_list = important_feature_names[:]\n",
    "    return [train_important_features,test_important_features,important_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7)Training the RandomForestRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Simple_Training_Algorithms(train_features,train_labels,test_features,test_label):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    models = []\n",
    "    \n",
    "    models.append(( 'LinearR' , LinearRegression()))\n",
    "\n",
    "    models.append(( 'SVR' , SVR()))\n",
    "\n",
    "    models.append(( 'RF' , RandomForestRegressor()))\n",
    "    \n",
    "    models.append(( 'GBR' , GradientBoostingRegressor()))\n",
    "    \n",
    "    models.append(( 'KNR' , KNeighborsRegressor()))\n",
    "\n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "\n",
    "    names = []\n",
    "    best_model_error = 1.7976931348623157e+308\n",
    "     \n",
    "    for name, model in models: \n",
    "        model.fit(train_features,train_labels)\n",
    "        y_pred = model.predict(test_features).round()\n",
    "        names.append(name)\n",
    "        error = mean_absolute_error(y_pred,test_label)\n",
    "        msg = \"%s: %f\" % (name, error)\n",
    "        print(msg)\n",
    "        if(best_model_error>error):\n",
    "            best_model_error = error\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "         \n",
    "    return [best_model,best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the default parameters being used in the RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_Tuning_And_Testing(train_features,train_labels,test_features,test_label):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "    models = []\n",
    "\n",
    "    models.append(( 'Base RF' , RandomForestRegressor(random_state = 0)))\n",
    "    \n",
    "    models.append(( 'N_estimators =100 RF' , RandomForestRegressor(n_estimators = 100,random_state = 0)))\n",
    "    \n",
    "    models.append(( 'N_estimators =1000 RF' , RandomForestRegressor(n_estimators = 1000)))\n",
    "\n",
    "    models.append(( 'N_estimators RF,verbose ' , RandomForestRegressor(n_estimators = 1000,random_state  = 0,warm_start  = True)))\n",
    "    \n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "\n",
    "    names = []\n",
    "    best_model_error = 1.7976931348623157e+308\n",
    "     \n",
    "    for name, model in models: \n",
    "        model.fit(train_features,train_labels)\n",
    "        y_pred = model.predict(test_features).round()\n",
    "        names.append(name)\n",
    "        error = mean_absolute_error(y_pred,test_label)\n",
    "        msg = \"%s: %f\" % (name, error)\n",
    "        print(msg)\n",
    "        if(best_model_error>error):\n",
    "            best_model_error = error\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "         \n",
    "    return [best_model,best_model_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RandomForestRegressor using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_for_optimal_parameters(features,labels):# Fitting Random Forest Classification to the Training set\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    from pprint import pprint\n",
    "\n",
    "    # Look at parameters used by our current forest\n",
    "    print('Parameters currently in use:\\n')\n",
    "    pprint(rf.get_params())\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    print(\"Numer of Trees:\",n_estimators)\n",
    "    \n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    print(\"Numer of features to consider at split:\",max_features)\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "     print(\"Maximum number of levels in tree:\",max_depth)\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    print(\"Minimum number of samples requires to split node\",min_samples_split)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "     print(\"Minimum number of samples required at leaf node\",min_samples_leaf)\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    #rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                                  n_iter = 10, scoring='neg_mean_absolute_error', \n",
    "                                  cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(features, labels)\n",
    "    rf_random.best_params_ \n",
    "    return rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8:Evaluation of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    predictions = model.predict(test_features).round()\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print(\"MAE:\",mean_absolute_error(predictions,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_forest_performance(X_train,X_test,y_train,y_test):\n",
    "    base_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Best Random Search Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9:Predict test data and write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_on_test(file,best_random,data_test,important_feature_names):\n",
    "    data_test = data_test[important_feature_names]\n",
    "    write_test_pred = best_random.predict(data_test).round()\n",
    "    arange = np.arange(1,write_test_pred.size+1)\n",
    "    df = pd.DataFrame({\"id\" : arange, \"prediction\" : write_test_pred})\n",
    "    df.to_csv(file, index=False)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(range(0,len(write_test_pred)),write_test_pred,color='blue')\n",
    "    plt.xlabel(\"Number of Predictions\")\n",
    "    plt.ylabel(\"Quality Prediction\")\n",
    "    plt.savefig(\"predict_test_plot.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data (5150, 13)\n",
      "Test Data (1347, 13)\n"
     ]
    }
   ],
   "source": [
    "train_file = 'train_data.csv'\n",
    "test_file ='test_data.csv'\n",
    "[original_train_data,original_test_data] = load_train_test_data(train_file,test_file)\n",
    "print(\"Train Data\",original_train_data.shape)\n",
    "print(\"Test Data\",original_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall empty data fields: 0\n"
     ]
    }
   ],
   "source": [
    "wrangle_data(original_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count        5150.00           5150.00      5150.00         5150.00   \n",
      "mean            7.22              0.34         0.32            5.44   \n",
      "std             1.30              0.17         0.15            4.81   \n",
      "min             3.80              0.08         0.00            0.60   \n",
      "25%             6.40              0.23         0.25            1.80   \n",
      "50%             7.00              0.29         0.31            2.90   \n",
      "75%             7.70              0.41         0.40            8.10   \n",
      "max            15.90              1.58         1.66           65.80   \n",
      "\n",
      "       chlorides  free sulfur dioxide  total sulfur dioxide  density       pH  \\\n",
      "count    5150.00              5150.00               5150.00  5150.00  5150.00   \n",
      "mean        0.06                30.63                115.71     0.99     3.22   \n",
      "std         0.04                18.03                 56.81     0.00     0.16   \n",
      "min         0.01                 1.00                  6.00     0.99     2.72   \n",
      "25%         0.04                17.00                 77.00     0.99     3.11   \n",
      "50%         0.05                29.00                118.00     0.99     3.20   \n",
      "75%         0.06                42.00                156.00     1.00     3.32   \n",
      "max         0.61               289.00                440.00     1.04     4.01   \n",
      "\n",
      "       sulphates  alcohol     type  quality  \n",
      "count    5150.00   5150.0  5150.00  5150.00  \n",
      "mean        0.53     10.5     1.75     5.82  \n",
      "std         0.15      1.2     0.43     0.88  \n",
      "min         0.22      8.0     1.00     3.00  \n",
      "25%         0.43      9.5     2.00     5.00  \n",
      "50%         0.51     10.3     2.00     6.00  \n",
      "75%         0.60     11.3     2.00     6.00  \n",
      "max         2.00     14.9     2.00     9.00  \n",
      "[5 6 7 4 8 3 9]\n",
      "6    2248\n",
      "5    1678\n",
      "7     872\n",
      "4     173\n",
      "8     149\n",
      "3      26\n",
      "9       4\n",
      "Name: quality, dtype: int64\n",
      "count    5150.000000\n",
      "mean        5.821359\n",
      "std         0.875161\n",
      "min         3.000000\n",
      "25%         5.000000\n",
      "50%         6.000000\n",
      "75%         6.000000\n",
      "max         9.000000\n",
      "Name: quality, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "analyse_dataset(original_train_data,\"quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA_histogram(original_train_data)\n",
    "#EDA_corr(original_train_data)\n",
    "#EDA_boxplot(original_train_data)\n",
    "#EDA_barplot(original_train_data)\n",
    "#EDF_Pairplot(original_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (3862, 12)\n",
      "Training Labels Shape: (3862,)\n",
      "Testing Features Shape: (1288, 12)\n",
      "Testing Labels Shape: (1288,)\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.25\n",
    "[train_features, test_features, train_labels, test_labels, feature_list] = split_data_into_test_train(original_train_data,split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: alcohol              Importance: 0.26\n",
      "Variable: volatile acidity     Importance: 0.13\n",
      "Variable: free sulfur dioxide  Importance: 0.09\n",
      "Variable: residual sugar       Importance: 0.07\n",
      "Variable: chlorides            Importance: 0.07\n",
      "Variable: total sulfur dioxide Importance: 0.07\n",
      "Variable: pH                   Importance: 0.07\n",
      "Variable: sulphates            Importance: 0.07\n",
      "Variable: fixed acidity        Importance: 0.06\n",
      "Variable: citric acid          Importance: 0.06\n",
      "Variable: density              Importance: 0.06\n",
      "Variable: type                 Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "determine_feature_importance(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important train features shape: (3862, 10)\n",
      "Important test features shape: (1288, 10)\n"
     ]
    }
   ],
   "source": [
    "[train_important_features,test_important_features,important_feature_names] = restrict_features_of_data(train_features,test_features,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearR: 0.536491\n",
      "SVR: 0.518634\n",
      "RF: 0.420807\n",
      "GBR: 0.479037\n",
      "KNR: 0.597050\n",
      "Best Model is  RF\n"
     ]
    }
   ],
   "source": [
    "[best_random,model_name] = test_Simple_Training_Algorithms(train_important_features,train_labels,test_important_features,test_labels)\n",
    "print(\"Best Model is \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base RF: 0.434783\n",
      "N_estimators =100 RF: 0.386646\n",
      "N_estimators =1000 RF: 0.385093\n",
      "N_estimators RF,verbose : 0.378106\n"
     ]
    }
   ],
   "source": [
    "[best_random,model_name] = RandomForest_Tuning_And_Testing(train_important_features,train_labels,test_important_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_random = random_search_for_optimal_parameters(train_important_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.4208 degrees.\n",
      "Accuracy = 92.37%.\n",
      "MAE: 0.42080745341614906\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_random, test_important_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e82cb72e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_model_on_test(\"new_submission.csv\",best_random,original_test_data,important_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
